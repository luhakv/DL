{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Юнит 8. Введение в нейронные сети\n",
    "### Skillfactory: DSPR-19\n",
    "### DL-1. Введение в Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Введение\n",
    "\n",
    "В первом модуле мы рассмотрим:\n",
    "\n",
    "- как строить нейронные сети;\n",
    "- какие типы нейронных сетей бывают;\n",
    "- где сегодня применяются нейронные сети.\n",
    "\n",
    "### 2. Машинное обучение и типы данных\n",
    "\n",
    "Прежде чем переходить к Deep Learning (глубокому обучению), актуализируем информацию по машинному обучению.\n",
    "\n",
    "Машинное обучение с высоты птичьего полёта можно рассмотреть как некоторое отображение входных данных в выходные данные. Как правило, этим занимается некоторая модель, которая зависит от заданных параметров. Эти параметры характеризуют то, как модель себя ведёт, как она предсказывает выходные значения по входным значениям. Подстройка этих параметров и есть машинное обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КЛАССИЧЕСКОЕ ОБУЧЕНИЕ С УЧИТЕЛЕМ \n",
    "\n",
    "Как правило, в машинном обучении используется **обучение с учителем (supervised learning)**, в котором у нас есть база данных, состоящая из пар объект – ответ. Мы показываем нашей модели эти пары одну за одной и корректируем параметры таким образом, чтобы на новых примерах наши модели предсказали хорошие правильные ответы. \n",
    "\n",
    "### КАКИЕ ТИПЫ ДАННЫХ МОГУТ ИСПОЛЬЗОВАТЬСЯ В ТАКИХ ЗАДАЧАХ? \n",
    "\n",
    "1. _Низкоразмерная информация_.  \n",
    "Например, это могут быть вектора, отвечающие за характеристики пользователей (рост, вес, возраст и т.д.), и мы хотим сделать их классификацию: то есть отображать низкоразмерный вектор в другой низкоразмерный вектор или скаляр.   \n",
    "2. _Изображение или видео._  \n",
    "Это более сложный тип данных. В качестве примера здесь можно рассмотреть задачу компьютерного зрения: отображение визуальной информации в низкоразмерную высокоуровневую информацию.   \n",
    "3. _Текст._  \n",
    "Пример: обработка текста и его отображение в качестве низкоразмерной высокоуровневой информацию.\n",
    "Аудио и звуковые сигналы.  \n",
    "\n",
    "Всем знакомый пример: распознавание речи и перевод аудио в текст.\n",
    "Мы можем делать практически любое отображение из любого типа данных в любой тип данных. При этом на каждое такое отображение есть соответствующая задача и методы её решения. Большинство этих задач наилучшим образом решаются с помощью нейронных сетей, или технологии Deep Learning. \n",
    "\n",
    "Важно понимать, что нейронные сети не всегда являются лучшим решением. Например, отображение низкоуровневой информации в саму себя лучше реализовать с помощью других алгоритмов (случайный лес, градиентный бустинг и др.).  Но в случае с высокоразмерными сложными данными (изображения, звуки, тексты) очень хорошо работают именно нейронные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.1\n",
    "В каких задачах Deep Learning имеет преимущество над классическим ML?  \n",
    "\n",
    "Ответ:\n",
    "- Преобразование звука в текст\n",
    "- Классификация текстовых строк"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Нейронные сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея нейронных сетей — это обучение представления. Получение выхода из заданного входа начинается с извлечения признаков, когда из входных данных извлекается некоторое промежуточное представление.\n",
    "\n",
    "### МНОГОСЛОЙНЫЙ ПЕРЦЕПТРОН \n",
    "\n",
    "Многослойный перцептрон — это простая нейронная сеть, которая состоит из:\n",
    "- входного вектора;\n",
    "- выходного вектора;\n",
    "- вектора промежуточного представления (скрытый слой).  \n",
    "\n",
    "Вычисление распространяется от входа к выходу, связям между нейронами соответствуют некоторые веса. Поэтому такая сеть является полносвязной.\n",
    "\n",
    "### ЧТО В НЕЙРОНЕ? \n",
    "\n",
    "В нейрон входит несколько значений x1, x2, x3 с несколькими связями. Связям соответствуют некоторые коэффициенты w1, w2, w3 (если у нас три входа в нейрон). \n",
    "\n",
    "Дальше внутри нейрона происходит вычисление двух операций, а точнее композиция линейной и нелинейной операции:\n",
    "\n",
    "- В линейной операции мы делаем взвешивание суммы всех входных значений (x1 умножаем на w1, x2 на w2 и так далее), всё это вместе суммируем и прибавляем некоторое значение смещения b. \n",
    "- В нелинейной операции от полученных на предыдущем шаге значений мы берём нелинейную функцию.  \n",
    "\n",
    "Так вычисляется выходное значение в одном нейроне. Дальше эти нейроны можно уже агрегировать в большую нейронную сеть.\n",
    "\n",
    "Одному слою такой нейронной сети уже будет соответствовать некоторая матрица параметров и некоторый вектор смещения B.\n",
    "\n",
    "Таким образом проходят вычисления в одном слое полносвязной нейронной сети и вычисляется матричное умножение и прибавление вектора и потом взятие поэлементно нелинейности. \n",
    "\n",
    "### КАК РАБОТАЮТ НЕЙРОНЫ? \n",
    "\n",
    "В одном нейроне нейронной сети у нас происходит локальное принятие решения: мы взвешиваем и суммируем входные данные и на основе полученных результатов локально что-то предсказываем, принимая маленькое решение на каждом шаге. Все полученные решения агрегируются и подаются на следующий слой, и уже новые значения используются для более сложного высокоуровнего принятия решений. \n",
    "\n",
    "В математическом смысле нейронная сеть — это универсальный _аппроксиматор*_ (может аппроксимировать любую функцию).\n",
    "\n",
    "Если есть какая-то сложная зависимость между входом и выходом, то можно с помощью нейронной сети описать эту зависимость. \n",
    "\n",
    "Если брать больше слоёв, то это уже будет многослойная нейронная сеть, и у неё уже будет более сложное промежуточное представление.\n",
    "\n",
    "* Аппроксима́ция (от лат. proxima — ближайшая), или приближе́ние — научный метод, состоящий в замене одних объектов другими, в каком-то смысле близкими к исходным, но более простыми.\n",
    "\n",
    "### ВЫЧИСЛЕНИЯ В МНОГОСЛОЙНОЙ НЕЙРОННОЙ СЕТИ \n",
    "\n",
    "Вычисления в такой сети можно записать с помощью следующего рекуррентного соотношения в многослойной нейронной сети:\n",
    "\n",
    "**Выход каждого слоя** — это есть вход, умноженный на какую-то матрицу, плюс вектор смещения, и от всего этого берётся нелинейность. Все веса, которые соответствуют смещениям _bias_, мы назовём параметрами нашей модели.\n",
    "\n",
    "### ТИПЫ ПАРАМЕТРОВ НЕЙРОННЫХ СЕТЕЙ \n",
    "\n",
    "Обычно у нейронных сетей выделяют два типа параметров: **просто параметры (W)** и **гиперпараметры.**\n",
    "\n",
    "**Гиперпараметры** — параметры нашей системы, которые мы не обучаем, а «создаём руками», как конструкторы проектируют нейронную сеть. Мы определяем количество нейронов в слое, количество слоёв, какую функцию активации мы используем и так далее. Это мы задаём вручную, а вот параметры связи между нейронами и bias — это вещи, которые получаются автоматически в процессе обучения.\n",
    "\n",
    "### КАКУЮ ФУНКЦИЮ АКТИВАЦИИ ВЗЯТЬ? \n",
    "\n",
    "Долгое время использовались **сигмоидальные функции, или sigmoid**. В этой функции мы помогаем выходу из нейрона принять какое-то бинарное решение. То есть отображаем все его значения во что-то больше или меньше нуля. Главное — что это _нелинейная функция_. \n",
    "\n",
    "Если бы мы использовали просто обычную линейную функцию (или вообще не использовали бы никакую функцию), то композиция слоёв без нелинейности давала бы нам одну большую линейную операцию. И поэтому не имело бы никакого смысла настраивать много слоёв в нейронной сети: они все были бы эквиваленты какому-то одному слою. \n",
    "\n",
    "А так, если мы ставим нелинейность между слоями, наш аппроксиматор становится более сложным, он уже может аппроксимировать достаточно сложные функции. Это функция sigmoid. Она использовалась раньше, но у неё есть некоторые проблемы, и поэтому сейчас, как правило, используют **функцию ReLU** или её модификации.\n",
    "\n",
    "В этой функции всё, что меньше 0, мы зануляем, а всё, что больше 0, оставляем как есть. У этой функции очень простая производная, а именно производная этой функции будет участвовать в процессе обучения и в алгоритме обратного распространения ошибки. \n",
    "\n",
    "### КАК ПРИМЕНИТЬ НЕЙРОННУЮ СЕТЬ ДЛЯ ЗАДАЧИ КЛАССИФИКАЦИИ? \n",
    "\n",
    "Представим, что у нас есть некоторые объекты в признаковом пространстве. Объекты задаются тремя числами, и у нас есть три компонента этого вектора.\n",
    "\n",
    "Мы так построили нашу нейронную сеть, что у неё есть три входных нейрона, как раз соразмерно нашему входному вектору. И, например, мы хотим сделать бинарную классификацию на 2 класса: фиолетовый и оранжевый. Поэтому мы сделали два выходных нейрона в нашей сети. Допустим, мы её уже как-то обучили. Так как её теперь использовать?\n",
    "\n",
    "Ставим на вход сети наш сектор из трёх компонентов, делаем прямое распространение по тем формулам, которые описаны выше и получаем два значения на выходе. Они уже отвечают на вопрос, к какому классу принадлежит наш объект, но ещё после некоторого специального нормирующего преобразования мы получаем другие два числа: P1, P2. Именно они уже явно характеризуют вероятность принадлежности к одному или второму классу. На выходе нейронной сети в случае классификации — распределение вероятностей принадлежности к тому или иному классу. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.2\n",
    "Зачем нужен вектор смещения B?   \n",
    "\n",
    "Ответ: Для сдвига нелинейной функции верно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Как обучить нейронную сеть?\n",
    "\n",
    "Нейронные сети обучаются с помощью обучения с учителем или на примерах. Для этого нам нужна обучающая выборка, которая состоит из пар входной объект — **выходной объект.** Мы подаём эту обучающую выборку в процесс обучения, который состоит в том, чтобы найти такие параметры модели W, чтобы наша нейронная сеть предсказывала правильно те самые ответы, которые мы уже знаем.\n",
    "\n",
    "Таким образом мы решаем задачу минимизации или задачу оптимизации: мы минимизируем ошибку на тех примерах, которые есть в нашей обучающей выборке.\n",
    "\n",
    "Ошибку можно записать по-разному:\n",
    "\n",
    "Здесь она записана как разница между правильным ответом D и предсказанием сети GW(Z). От этой разницы мы берём норму и получаем скаляр, который мы хотим минимизировать по всей обучающей выборке. Затем мы ищем такие веса (нижнее соотношение W* ), которые минимизируют эту ошибку. \n",
    "\n",
    "### КАК РЕШАТЬ ЗАДАЧУ ОПТИМИЗАЦИИ? \n",
    "\n",
    "Есть различные способы. В теории оптимизации есть такой известный алгоритм как градиентный спуск. Смысл объясним на примере. \n",
    "\n",
    "Допустим, у вас есть какая-то функция (здесь представлена одномерная функция, но в общем случае она может быть и многомерной), и вы хотите найти её минимум. Вы стоите в некоторой точке, и вам нужно понять, в какую сторону вам с этой точки нужно сдвинуться, чтобы приблизиться к минимуму. Есть вектор, который называется градиент. И этот вектор смотрит в сторону возрастания функции. Поэтому градиент со знаком минус смотрит в сторону убывания функции.\n",
    "\n",
    "Подобный алгоритм предлагает двигаться в сторону антиградиента и таким образом приближаться к локальному минимуму. \n",
    "\n",
    "Итак, мы вычислили градиент, и с некоторым параметром α, который ещё называют Learning rate (скорость обучения), мы этот антиградиент прибавляем к нашим текущем весам, и так получается итерационное движение к минимуму. \n",
    "\n",
    "В случае нейронных сетей используется небольшая модификация — **стохастический градиентный спуск.** Отличие от предыдущего градиентного спуска в том, что мы вычисляем градиент не сразу на всех образцах нашей выборки, а только на одном образце за одну итерацию или на группе образцов.\n",
    "\n",
    "### КАК ВЫЧИСЛИТЬ ГРАДИЕНТ? \n",
    "\n",
    "Это вектор или даже некоторый тензор, размерность которого совпадает с размерностью всех наших параметров обучаемых.\n",
    "\n",
    "Градиент может особенно зависеть от слоёв, которые далеки от той самой ошибки, от которой мы считаем градиент. Например, у нас ошибка зависит от параметров первого слоя очень сложным образом, и, тем не менее, мы всё равно можем вычислить градиент с помощью алгоритма обратного распространения ошибки, который заключается в том, что мы используем  правила дифференцирования сложной функции.\n",
    "\n",
    "Нейронная сеть, даже если она представляет собой сложную функцию — на самом деле просто композиция каких-то маленьких простых вещей. Например, умножили на матрицу, прибавили вектор, взяли поэлементную матрицу и так далее. Используя такое дифференцирование сложной функции, можно узнать, как наша ошибка зависела от параметров второго слоя через это цепное правило. Именно таким образом можно вычислить градиент по любому весу и по всем весам. Это обратное распространение ошибки, или ещё его называют backpropagation.\n",
    "\n",
    "### ДЛЯ КАКИХ ЗАДАЧ ПРИМЕНЯТЬ? \n",
    "\n",
    "- классификация;\n",
    "- регрессия;\n",
    "- машинное зрение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.1\n",
    "Чем стохастический градиентный спуск отличается от обычного?  \n",
    "\n",
    "Ответ: Градиент вычисляется для случайно отобранных примеров "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.2\n",
    "Что такое градиент?  \n",
    "\n",
    "Ответ: Величина, используемая для вычисления изменения весов нейросети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Задача компьютерного зрения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача компьютерного зрения состоит в отображении визуальной информации (изображения или видео) в высокоуровневую, семантическую информацию. \n",
    "\n",
    "В классификации изображений на входе находится картинка, на выходе — метка класса. \n",
    "\n",
    "При детектировании и локализации объектов, помимо классификации, даётся ограничивающий прямоугольник, или локализация, где тот или иной объект присутствует на изображении.\n",
    "\n",
    "**Семантическая сегментация** — более сложная задача, в ней классифицируется каждый пиксель изображения, или выделяются некоторые сегменты на изображении, и каждому сегменту присваивается какая-то метрика.\n",
    "рисунок\n",
    "\n",
    "Основная трудность этих задач в том, что картинка или изображение представляет собой огромное количество неструктурированной информации, к которой непонятно, как подойти: это просто какие-то числа RGB. В машинном зрении ещё до прихода нейронных сетей использовались признаки. Признаки — это то, что  описывает изображения. На картинке находятся какие-то особенности, они записываются в виде уже осмысленного высокоуровневого вектора и дальше к этому вектору применяется алгоритм классификации и получается ответ. \n",
    "\n",
    "В итоге решение задачи сводится к двум частям:\n",
    "\n",
    "- извлечение признаков;\n",
    "- подача признаков в алгоритм машинного обучения.\n",
    "\n",
    "До появления машинного зрения все признаки создавались вручную специальными инженерами, которые анализировали изображение и вручную кодировали то, как признаки должны выглядеть.\n",
    "\n",
    "**Свёртка** — это способ извлечения признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5.1\n",
    "Для извлечения признаков используется:  \n",
    "\n",
    "Ответ: свёртка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Где взять ядро свёртки?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея свёрточных нейронных сетей состоит в том, чтобы использовать свёртку как специальный тип слоя в нейронной сети.\n",
    "\n",
    "Обратимся к предыдущему примеру:\n",
    "\n",
    "Посмотрим на свёртку: здесь видно, что у нас используется некоторая линейная операция для вычисления значения в выходной матрице. Такая же линейная операция использовалась нами в нейронных сетях. \n",
    "\n",
    "Вернемся:\n",
    "\n",
    "Разница в том, что нейроны уже связаны в соседних тензорах. Не каждый с каждым, а только какая-то группа нейронов во входном слое связана с одним нейроном в выходном слое. Именно так и делается в свёрточных нейронных сетях. \n",
    "\n",
    "Веса в ядре свёртки получаются с помощью оптимизации. Теперь они соответствуют связям между нейронами. Таким образом, мы получили автоматически обучаемые ядра свёрток или, другими словами, **обучаемые признаки** (в отличие от тех признаков в компьютерном зрении, которые использовались ранее, когда люди задавали их вручную).\n",
    "\n",
    "**Основная идея глубокого обучения** — не просто изучение признаков, а извлечение их иерархии: извлечение признаков вышестоящего уровня в пространстве нижестоящего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6.1\n",
    "Выберите идеи, присущие именно глубокому обучению:  \n",
    "\n",
    "Ответ:\n",
    "- Автоматическое извлечение признаков\n",
    "- Извлечение всё более крупных и абстрактных признаков при движении вверх по иерархии\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Что такое Deep Learning? Типы слоёв\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning** —  это обучение иерархии признаковых представлений.   \n",
    "\n",
    "Что здесь важно: \n",
    "\n",
    "- Тут есть признаки: сначала из входных данных извлекаются признаки, и дальше работа идёт уже с ними .\n",
    "- Тут есть обучаемые признаки: мы делаем так, чтобы нейронная сеть их учила автоматически.\n",
    "- Тут не просто один слой признаков — здесь иерархия признаков.  \n",
    "\n",
    "**Какие типы слоёв бывают в таких сетях?**\n",
    "\n",
    "- свёрточный слой;\n",
    "- понижение размерности (Pooling) \n",
    "- локальное усреднение;\n",
    "- локальный максимум;\n",
    "- полносвязный слой (Fully-connected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 7.1\n",
    "Слой pooling нужен для:\n",
    "\n",
    "Ответ:\n",
    "- получения инвариантного представления признаков\n",
    "- снижения размерности данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Обработка последовательностей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обработка последовательности** с использованием рекуррентных нейронных сетей состоит в том, что после каждой обработки какого-то элемента последовательности, сеть запоминает все, что уже было до этого. \n",
    "То есть нейронная сеть обладает своеобразной внутренней памятью.\n",
    "\n",
    "**Более сложные рекуррентные сети:**\n",
    "\n",
    "LSTM (https://clck.ru/MJ47X);\n",
    "GRU (https://clck.ru/MJXa7).  \n",
    "\n",
    "Где применяются рекуррентные нейронные сети:\n",
    "\n",
    "- умная клавиатура;\n",
    "- анализ комментариев;\n",
    "- машинный перевод;\n",
    "- чат-боты;\n",
    "- распознавание речи;\n",
    "- отображение картинок в текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 8.1\n",
    "Рекуррентные сети подходят для:\n",
    "\n",
    "Ответ:\n",
    "- классификации текстов\n",
    "- преобразования голоса в текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 8.2\n",
    "Что передаёт рекуррентный модуль на вход самому себе?  \n",
    "\n",
    "Ответ:\n",
    "- Скрытое состояние (Hidden State) верно\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. От распознания к синтезу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Синтез** — это обратная задача, то есть перевод информации высокого уровня в информацию низкого уровня. \n",
    "На примере человека — восприятие и творчество:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На примере работы техники— распознавание и синтез:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 9.1\n",
    "Перенос стиля — это:\n",
    "слияние текстурных и обычных признаков картинок "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Состязательные сети\n",
    "\n",
    "**Состязательная сеть** (англ. _Generative Adversarial Networks_, **GAN**).  \n",
    "\n",
    "Идея состоит в том, чтобы построить вторую сеть, которая называется **дискриминатор**. Это просто бинарный классификатор: на входе — картинка, а на выходе — ответ с предположением, реальная картинка или синтезированная. \n",
    "\n",
    "Важный момент: генератор должен учиться обманывать дискриминатор. Они учатся параллельно, играя в антагонистическую игру, и в итоге мы получаем идеальный генератор и идеальный дискриминантор.\n",
    "\n",
    "Примеры применения:\n",
    "\n",
    "- синтез изображений;\n",
    "- преобразование текста в изображение;\n",
    "- отображение изображения в изображение;\n",
    "- отображение видео в видео;\n",
    "- синтез речи (Wavenet);\n",
    "- аудио в видео.  \n",
    "\n",
    "### Задание 10.1\n",
    "Дискриминатор в состязательной сети:\n",
    "- формирует данные для градиента генератора\n",
    "- учится отличать сгенерированные картинки от настоящих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
