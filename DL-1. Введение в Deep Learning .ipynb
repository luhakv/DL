{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Юнит 8. Введение в нейронные сети\n",
    "### Skillfactory: DSPR-19\n",
    "### DL-1. Введение в Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Введение\n",
    "\n",
    "В первом модуле мы рассмотрим:\n",
    "\n",
    "- как строить нейронные сети;\n",
    "- какие типы нейронных сетей бывают;\n",
    "- где сегодня применяются нейронные сети.\n",
    "\n",
    "### 2. Машинное обучение и типы данных\n",
    "\n",
    "Прежде чем переходить к Deep Learning (глубокому обучению), актуализируем информацию по машинному обучению.\n",
    "\n",
    "Машинное обучение с высоты птичьего полёта можно рассмотреть как некоторое отображение входных данных в выходные данные. Как правило, этим занимается некоторая модель, которая зависит от заданных параметров. Эти параметры характеризуют то, как модель себя ведёт, как она предсказывает выходные значения по входным значениям. Подстройка этих параметров и есть машинное обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КЛАССИЧЕСКОЕ ОБУЧЕНИЕ С УЧИТЕЛЕМ \n",
    "\n",
    "Как правило, в машинном обучении используется **обучение с учителем (supervised learning)**, в котором у нас есть база данных, состоящая из пар объект – ответ. Мы показываем нашей модели эти пары одну за одной и корректируем параметры таким образом, чтобы на новых примерах наши модели предсказали хорошие правильные ответы. \n",
    "\n",
    "### КАКИЕ ТИПЫ ДАННЫХ МОГУТ ИСПОЛЬЗОВАТЬСЯ В ТАКИХ ЗАДАЧАХ? \n",
    "\n",
    "1. _Низкоразмерная информация_.  \n",
    "Например, это могут быть вектора, отвечающие за характеристики пользователей (рост, вес, возраст и т.д.), и мы хотим сделать их классификацию: то есть отображать низкоразмерный вектор в другой низкоразмерный вектор или скаляр.   \n",
    "2. _Изображение или видео._  \n",
    "Это более сложный тип данных. В качестве примера здесь можно рассмотреть задачу компьютерного зрения: отображение визуальной информации в низкоразмерную высокоуровневую информацию.   \n",
    "3. _Текст._  \n",
    "Пример: обработка текста и его отображение в качестве низкоразмерной высокоуровневой информацию.\n",
    "Аудио и звуковые сигналы.  \n",
    "\n",
    "Всем знакомый пример: распознавание речи и перевод аудио в текст.\n",
    "Мы можем делать практически любое отображение из любого типа данных в любой тип данных. При этом на каждое такое отображение есть соответствующая задача и методы её решения. Большинство этих задач наилучшим образом решаются с помощью нейронных сетей, или технологии Deep Learning. \n",
    "\n",
    "Важно понимать, что нейронные сети не всегда являются лучшим решением. Например, отображение низкоуровневой информации в саму себя лучше реализовать с помощью других алгоритмов (случайный лес, градиентный бустинг и др.).  Но в случае с высокоразмерными сложными данными (изображения, звуки, тексты) очень хорошо работают именно нейронные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.1\n",
    "В каких задачах Deep Learning имеет преимущество над классическим ML?  \n",
    "\n",
    "Ответ:\n",
    "- Преобразование звука в текст\n",
    "- Классификация текстовых строк"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Нейронные сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея нейронных сетей — это обучение представления. Получение выхода из заданного входа начинается с извлечения признаков, когда из входных данных извлекается некоторое промежуточное представление.\n",
    "\n",
    "### МНОГОСЛОЙНЫЙ ПЕРЦЕПТРОН \n",
    "\n",
    "Многослойный перцептрон — это простая нейронная сеть, которая состоит из:\n",
    "- входного вектора;\n",
    "- выходного вектора;\n",
    "- вектора промежуточного представления (скрытый слой).  \n",
    "\n",
    "Вычисление распространяется от входа к выходу, связям между нейронами соответствуют некоторые веса. Поэтому такая сеть является полносвязной.\n",
    "\n",
    "### ЧТО В НЕЙРОНЕ? \n",
    "\n",
    "В нейрон входит несколько значений x1, x2, x3 с несколькими связями. Связям соответствуют некоторые коэффициенты w1, w2, w3 (если у нас три входа в нейрон). \n",
    "\n",
    "Дальше внутри нейрона происходит вычисление двух операций, а точнее композиция линейной и нелинейной операции:\n",
    "\n",
    "- В линейной операции мы делаем взвешивание суммы всех входных значений (x1 умножаем на w1, x2 на w2 и так далее), всё это вместе суммируем и прибавляем некоторое значение смещения b. \n",
    "- В нелинейной операции от полученных на предыдущем шаге значений мы берём нелинейную функцию.  \n",
    "\n",
    "Так вычисляется выходное значение в одном нейроне. Дальше эти нейроны можно уже агрегировать в большую нейронную сеть.\n",
    "\n",
    "Одному слою такой нейронной сети уже будет соответствовать некоторая матрица параметров и некоторый вектор смещения B.\n",
    "\n",
    "Таким образом проходят вычисления в одном слое полносвязной нейронной сети и вычисляется матричное умножение и прибавление вектора и потом взятие поэлементно нелинейности. \n",
    "\n",
    "### КАК РАБОТАЮТ НЕЙРОНЫ? \n",
    "\n",
    "В одном нейроне нейронной сети у нас происходит локальное принятие решения: мы взвешиваем и суммируем входные данные и на основе полученных результатов локально что-то предсказываем, принимая маленькое решение на каждом шаге. Все полученные решения агрегируются и подаются на следующий слой, и уже новые значения используются для более сложного высокоуровнего принятия решений. \n",
    "\n",
    "В математическом смысле нейронная сеть — это универсальный _аппроксиматор*_ (может аппроксимировать любую функцию).\n",
    "\n",
    "Если есть какая-то сложная зависимость между входом и выходом, то можно с помощью нейронной сети описать эту зависимость. \n",
    "\n",
    "Если брать больше слоёв, то это уже будет многослойная нейронная сеть, и у неё уже будет более сложное промежуточное представление.\n",
    "\n",
    "* Аппроксима́ция (от лат. proxima — ближайшая), или приближе́ние — научный метод, состоящий в замене одних объектов другими, в каком-то смысле близкими к исходным, но более простыми.\n",
    "\n",
    "### ВЫЧИСЛЕНИЯ В МНОГОСЛОЙНОЙ НЕЙРОННОЙ СЕТИ \n",
    "\n",
    "Вычисления в такой сети можно записать с помощью следующего рекуррентного соотношения в многослойной нейронной сети:\n",
    "\n",
    "**Выход каждого слоя** — это есть вход, умноженный на какую-то матрицу, плюс вектор смещения, и от всего этого берётся нелинейность. Все веса, которые соответствуют смещениям _bias_, мы назовём параметрами нашей модели.\n",
    "\n",
    "### ТИПЫ ПАРАМЕТРОВ НЕЙРОННЫХ СЕТЕЙ \n",
    "\n",
    "Обычно у нейронных сетей выделяют два типа параметров: **просто параметры (W)** и **гиперпараметры.**\n",
    "\n",
    "**Гиперпараметры** — параметры нашей системы, которые мы не обучаем, а «создаём руками», как конструкторы проектируют нейронную сеть. Мы определяем количество нейронов в слое, количество слоёв, какую функцию активации мы используем и так далее. Это мы задаём вручную, а вот параметры связи между нейронами и bias — это вещи, которые получаются автоматически в процессе обучения.\n",
    "\n",
    "### КАКУЮ ФУНКЦИЮ АКТИВАЦИИ ВЗЯТЬ? \n",
    "\n",
    "Долгое время использовались **сигмоидальные функции, или sigmoid**. В этой функции мы помогаем выходу из нейрона принять какое-то бинарное решение. То есть отображаем все его значения во что-то больше или меньше нуля. Главное — что это _нелинейная функция_. \n",
    "\n",
    "Если бы мы использовали просто обычную линейную функцию (или вообще не использовали бы никакую функцию), то композиция слоёв без нелинейности давала бы нам одну большую линейную операцию. И поэтому не имело бы никакого смысла настраивать много слоёв в нейронной сети: они все были бы эквиваленты какому-то одному слою. \n",
    "\n",
    "А так, если мы ставим нелинейность между слоями, наш аппроксиматор становится более сложным, он уже может аппроксимировать достаточно сложные функции. Это функция sigmoid. Она использовалась раньше, но у неё есть некоторые проблемы, и поэтому сейчас, как правило, используют **функцию ReLU** или её модификации.\n",
    "\n",
    "В этой функции всё, что меньше 0, мы зануляем, а всё, что больше 0, оставляем как есть. У этой функции очень простая производная, а именно производная этой функции будет участвовать в процессе обучения и в алгоритме обратного распространения ошибки. \n",
    "\n",
    "### КАК ПРИМЕНИТЬ НЕЙРОННУЮ СЕТЬ ДЛЯ ЗАДАЧИ КЛАССИФИКАЦИИ? \n",
    "\n",
    "Представим, что у нас есть некоторые объекты в признаковом пространстве. Объекты задаются тремя числами, и у нас есть три компонента этого вектора.\n",
    "\n",
    "Мы так построили нашу нейронную сеть, что у неё есть три входных нейрона, как раз соразмерно нашему входному вектору. И, например, мы хотим сделать бинарную классификацию на 2 класса: фиолетовый и оранжевый. Поэтому мы сделали два выходных нейрона в нашей сети. Допустим, мы её уже как-то обучили. Так как её теперь использовать?\n",
    "\n",
    "Ставим на вход сети наш сектор из трёх компонентов, делаем прямое распространение по тем формулам, которые описаны выше и получаем два значения на выходе. Они уже отвечают на вопрос, к какому классу принадлежит наш объект, но ещё после некоторого специального нормирующего преобразования мы получаем другие два числа: P1, P2. Именно они уже явно характеризуют вероятность принадлежности к одному или второму классу. На выходе нейронной сети в случае классификации — распределение вероятностей принадлежности к тому или иному классу. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.2\n",
    "Зачем нужен вектор смещения B?   \n",
    "\n",
    "Ответ: Для сдвига нелинейной функции верно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
