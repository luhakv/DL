{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Юнит 8. Введение в нейронные сети\n",
    "### Skillfactory: DSPR-19\n",
    "### DL-2. Практика с основными фреймворками "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Уровни абстракции\n",
    "\n",
    "Самое время приступать к практике с основными фреймворками! В этом модуле мы научимся работать с двумя — **TensorFlow** и **PyTorch.**\n",
    "\n",
    "Но для начала разберёмся, насколько глубоко мы вообще можем погружаться в обучение нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует несколько уровней абстракции, на которых мы можем сконструировать нейросеть:\n",
    "\n",
    "- **Нижний уровень (NumPy)** — нет готовых высокоуровневых блоков, нет оптимизаторов, отсутствует возможность параллельных расчётов и использования GPU. Используется в учебных целях для иллюстрации принципов работы нейросетей.\n",
    "- **Фреймворки автоматического дифференцирования (PyTorch и TensorFlow)** — вы определяете вычислительный граф и некоторые гиперпараметры, а всё остальное фреймворк делает за вас. Позволяет строить нейросети произвольной сложности.\n",
    "- **Фреймворки готовых моделей (Transformers и DeepPavlov)** — скрывают технические детали самой нейросети и представляют высокоуровневый интерфейс для основных операций: обучения и инференса. Являются обёртками для больших и сложных нейросетей.\n",
    "\n",
    "### ФРЕЙМВОРКИ АВТОМАТИЧЕСКОГО ДИФФЕРЕНЦИРОВАНИЯ \n",
    "\n",
    "Рассмотрим, какие фреймворки представлены на рынке.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Вспоминаем линейную классификацию\n",
    "\n",
    "Прежде чем знакомиться с фреймворками, давайте снова поговорим о том, что такое линейная классификация. Это нам обязательно пригодится далее.\n",
    "\n",
    "### ЛИНЕЙНАЯ КЛАССИФИКАЦИЯ \n",
    "\n",
    "Представьте, что у нас есть признаки x = (x1, x2) и есть выборка положительных и отрицательных точек y ∈ {+1, −1}\n",
    "\n",
    "Нам нужно найти разделяющую гиперплоскость между ними. В данном случае это просто линия. Линия задаётся тремя коэффициентами:\n",
    "\n",
    "Нам нужно найти три коэффициента w, которые зададут линию. Далее мы можем взять точку х и понять, где она находится относительно линии: выше или ниже. Для этого нам нужно узнать знак линейной комбинации. Вектор весов w задаёт нормаль к нашей линии, то есть он перпендикулярен ей (фиолетовый вектор на графике ниже).\n",
    "\n",
    "Линейная комбинация — это скалярное произведение и длина проекции какого-нибудь другого вектора на наш вектор w.\n",
    "\n",
    "Поэтому проекция становится разных знаков. Из этих соображений мы делаем линейный классификатор. Наш алгоритм: \n",
    "\n",
    "Здесь появляется знак нашей линейной комбинации. Настроить линейный классификатор — значит найти эти коэффициенты. \n",
    "\n",
    "### ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ \n",
    "\n",
    "Она тоже решает задачу классификации, но в конце применяется не функция знака, а сигмоидная функция. Она превращает длину проекции в уверенность.\n",
    "\n",
    "Уверенность и неуверенность появляется из-за краевых эффектов: на границе классов может быть какой-то шум, и в классификации точек, которые находятся рядом с красной разделяющей линией, мы не очень уверены. \n",
    "\n",
    "А если мы уходим далеко от линии вглубь классов, то предполагается, что мы более уверены в этом предсказании. Сигмоида делает именно это — превращает длину проекции линейной комбинации в уверенность.\n",
    "\n",
    "Сигмоида устроена не случайным образом:\n",
    "\n",
    "Если длина проекции 0 (точка ровно на красной линии) , то сигмоида даёт . Логистическая регрессия предсказывает вероятность положительного класса. Вероятность отрицательного будет единица минус предсказанная вероятность положительного класса. \n",
    "\n",
    "#### ДРУГОЙ ПРИМЕР \n",
    "\n",
    "Представим, что у нас есть следующая задача:\n",
    "\n",
    "Чтобы решить подобную задачу, мы можем «подпереть» треугольник тремя линиями и сделать алгоритм, используя только логистическую регрессию.\n",
    "\n",
    "Для начала мы отделим минусы слева и построим логистическую регрессию:\n",
    "\n",
    "Эти данные подадим для обучения логистической регрессии и получим коэффициенты красной линии. \n",
    "\n",
    "Отделим остальные минусы:\n",
    "\n",
    "Все коэффициенты мы получили практически вручную. Сейчас у нас есть коэффициенты трёх логистических регрессий, каждая из которых решает свою маленькую подзадачу. \n",
    "\n",
    "Теперь возьмём какую-нибудь точку и посмотрим, какие три предсказания дают эти линии в точке:\n",
    "\n",
    "### ЧТО ДЕЛАТЬ С ЭТИМИ ТРЕМЯ ЗНАЧЕНИЯМИ? \n",
    "\n",
    "В координатах х1 и х2 эта задача не решается. Поэтому полученные коэффициенты мы можем рассматривать как новые координаты. \n",
    "\n",
    "Получим три признака, каждый из которых говорит, где мы находимся относительно каждой стороны треугольника. \n",
    "\n",
    "Давайте возьмём наш целевой признак y, добавим его в нашу новую таблицу, где наши новые признаки с предсказаниями, и попробуем решить её с новыми признаками с помощью линейной логистической регрессии. На новой выборке получим логистическую регрессию:\n",
    "\n",
    "Теперь она даёт нам некоторые коэффициенты и взвешивает уже не признаки, а предсказания. То, что мы получили — простейшая нейросеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.1\n",
    "Почему в логистической регрессии используется Сигмоида, а не Знак (sign)?  \n",
    "\n",
    "Ответ: Сигмоида плавная и непрерывная, что позволяет её дифференцировать верно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Граф вычислений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент мы «руками» нашли параметры всех линий:\n",
    "\n",
    "При этом на нашу сложную комбинацию функций можно посмотреть как на граф вычислений. У графа есть вершины, и каждая вершина — вычисляемое значение.\n",
    "\n",
    "Также есть рёбра — зависимости, которые имеют направления (на картинке ниже это стрелочки). Ребро идёт от х1 к z1 в случае, если нам необходим х1, чтобы вычислить значение z1. Это граф зависимости между вычисляемыми значениями:\n",
    "\n",
    "Граф соответствует комбинации наших функций. Такой граф называют многослойным персептроном, и здесь уже можно видеть некоторые слои:\n",
    "\n",
    "- входной слой (признаки);\n",
    "- скрытый слой (нейроны);\n",
    "- выходной слой (предсказания).\n",
    "\n",
    "### НЕЛИНЕЙНОСТИ В НЕЙРОНАХ \n",
    "\n",
    "Возьмём в качестве примера следующий граф:\n",
    "\n",
    "Если нелинейности убрать, то на этом примере видно, что наша модель станет очень простой: мы можем подставить выражения для z1 и z2 в нашу модель а:\n",
    "\n",
    "Мы можем раскрыть скобки, привести подобные слагаемые и всё, что мы получим — линейную комбинацию х1 и х2. При этом модель сложнее не становится.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.1\n",
    "Зачем перцептроны делают многослойными?  \n",
    "\n",
    "Ответ: Несколько слоёв позволяют разделить данные, которые не делятся одной линией "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.2\n",
    "Что будет, если убрать нелинейность из скрытого слоя?  \n",
    "\n",
    "\n",
    "Ответ:  \n",
    "- Скрытого слоя как бы не станет\n",
    "- Модель не сможет работать со сложными данными, не делящимися одной линией\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MLP\n",
    "\n",
    "**MLP** — это простейший пример нейросети:\n",
    "\n",
    "Слои в MLP называют:\n",
    "\n",
    "- Dense layer (плотный);\n",
    "- Fully-connected layer (полносвязный).\n",
    "\n",
    "### Архитектура MLP:\n",
    "\n",
    "- количество слоёв;\n",
    "- количество нейронов в каждом слое;\n",
    "- функция активации, которую будем использовать.\n",
    "\n",
    "\n",
    "### Задание 4.1\n",
    "\n",
    "Какие из параметров обучаемые, а какие являются гиперпараметрами нейросети?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр | Вид\n",
    ":-- | :--\n",
    "__Веса связей между входными нейронами и слоем Z__ |\tОбучаемые параметры \n",
    "__Количество нейронов в слое Z__ |\tгиперпараметры\n",
    "__Функция активации в слое Z__ |\tгиперпараметры\n",
    "__Веса связей между слоем Z и слоем H__ |\tОбучаемые параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. TensorFlow\n",
    "\n",
    "**TENSORFLOW (TF) — DEEP LEARNING ФРЕЙМВОРК**\n",
    "\n",
    "Основа вычислений в **TF — граф**. Каждая вершина графа — это одна операция, у которой есть входы и выходы.\n",
    "\n",
    "- Вход любой операции — это набор тензоров (многомерных массивов).\n",
    "- Выход любой операции — это тоже набор тензоров.  \n",
    "\n",
    "А у нас целый граф операций, между которыми перекидываются тензоры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительные материалы:\n",
    "https://www.oreilly.com/content/hello-tensorflow/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5.1\n",
    "Тензором являются:\n",
    "- данные, поступающие на вход\n",
    "- веса связей между слоями\n",
    "- ошибка предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Пример обучения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь обычно дифференцируемая. Если представить, что у нас есть параметр w, который нужно оптимизировать, то мы можем начать с любой точки (initial weight) и посчитать в ней производную. В данном случае это tg наклона касательной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве дополнительной литературы рекомендуем ознакомиться с визуализацией обучения нейронной сети с помощью **TensorFlow Playground** (http://playground.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6.1\n",
    "Какое минимальное количество нейронов должно быть в скрытом слое, чтобы решить задачу Exclusive OR с test loss < 0.1?\n",
    "(задача Exclusive OR): Exclusive OR  \n",
    "    \n",
    "Ответ: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6.2\n",
    "Почему задача Gaussian всегда решается с хорошим качеством (test loss < 0.01), а задача Exclusive OR не всегда сходится, даже при достаточном количестве нейронов скрытого слоя?  \n",
    "\n",
    "(задача Gaussian): Exclusive OR\n",
    "\n",
    "(задача Exclusive OR): Exclusive OR  \n",
    "\n",
    "Ответ: Задача Exclusive OR имеет несколько минимумов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Детали градиентного спуска: цепное правило\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 7.1\n",
    "Обязательно ли применять цепное правило для расчёта производной функции?  \n",
    "\n",
    "Ответ: да, нет, да, нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Граф вычисления производных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### АЛГОРИТМ ВЫЧИСЛЕНИЯ ПРОИЗВОДНОЙ В ГРАФЕ \n",
    "\n",
    "Как посчитать производную a по b:\n",
    "\n",
    "- Находим непосещённый путь из a в b.  \n",
    "- Перемножаем значения на рёбрах в пути.  \n",
    "- Добавляем в сумму.  \n",
    "- Возвращаемся к п.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 8.1\n",
    "Сколько слагаемых будет содержать производная графа из 2 скрытых слоёв, если в первом скрытом слое 3 нейрона, а во втором — 2?  \n",
    "\n",
    "Ответ: 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Пример одного нейрона\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, из чего он состоит по порядку:\n",
    "\n",
    "x1, x2 — признаки. Это то, что подаётся на вход (плейсхолдеры);  \n",
    "α, β — веса модели (переменные);  \n",
    "звездочка*  — операции произведения (после идёт подсчёт суммы);  \n",
    "s — применяем сигмоиду;  \n",
    "z — записываем в новую переменную;  \n",
    "L — функция потерь, которая берёт на вход правильные ответы y, предсказанные ответы z  и говорит, насколько они близки друг к другу;  \n",
    "y — плейсхолдер, правильный ответ.  \n",
    "\n",
    "Производные функции нам нужны, чтобы узнать как нужно изменить α и β, чтобы минимизировать наши потери: \n",
    "\n",
    "Для того чтобы взять производную, мы переворачиваем наш граф:\n",
    "\n",
    "\n",
    "\n",
    "Для каждой стрелочки считаем производную. По сути нам нужно найти обратный путь от L до α:\n",
    "\n",
    "Так же считается и по β:\n",
    "\n",
    "В TF  все производные строятся автоматически и достаточно быстро. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 9.1\n",
    "Как изменится схема нашего графа, если вместо задачи бинарной классификации мы будем делать многоклассовую классификацию с тремя классами?  \n",
    "\n",
    "Ответ:\n",
    "- Количество выходных нейронов увеличится до 3\n",
    "- Изменится функция потерь (Это будет кроссэнтропия)\n",
    "- Изменится функция активации на выходном слое (Это будет softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Цепное правило и граф производных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть алгоритм для подсчёта производных для любых дифференцируемых графов вычислений. Эффективный способ вычисления всех производных называется back-propagation (обратное распространение ошибки):\n",
    "\n",
    "### BACK-PROPAGATION (BACK-PROP) \n",
    "\n",
    "В Back-propogation есть два прохода: прямой и обратный. Те производные, которые считает обратный граф, нужно считать в определённой точке. Именно для этого нужен прямой проход — он рассчитает аргументы всех этих производных:\n",
    "\n",
    "### ИНИЦИАЛИЗАЦИЯ ВЕСОВ \n",
    "\n",
    "Мы не можем инициализировать нулями. \n",
    "\n",
    "Если на данном примере мы стартуем на нулевых весах, то w2 и w3 будут обновляться симметрично, а значит, мы не научим никакую сложную комбинацию за счёт простоты сети. Это называют симметрией. \n",
    "\n",
    "Чтобы сломать симметрию, мы можем использовать случайный шум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «Step-by-step backpropagation example» (https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 10.1\n",
    "Зачем нужен прямой проход в backpropagation?  \n",
    "\n",
    "Ответ: Чтобы рассчитать значения активаций каждого нейрона "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Практика. TF в Google Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемый в видео notebook: skillfactory-dl-2-screencast.ipynb. Notebook использует устаревшую версию TensorFlow, и изучать его не нужно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом видео мы познакомимся со средой Google Colab и рассмотрим, что можно делать в TF.\n",
    "\n",
    "Google Colab (https://colab.research.google.com/) — сервис, который позволяет запускать Juputer-ноутбуки, в которых есть видеокарты и много оперативной памяти.\n",
    "Перейдём на страницу https://colab.research.google.com/notebooks/welcome.ipynb.\n",
    "\n",
    "Рекомендуем вам залогиниться через аккаунт Google.\n",
    "Используемый в видео notebook skillfactory-dl-2-screencast.ipynb сразу можно открыть в Google Colab.\n",
    "\n",
    "Вы можете сразу изучить все комбинации клавиш во вкладке Tools — Keyboard chortcuts, например: Ctrl + Enter для запуска ячейки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Практика\n",
    "\n",
    "Ноутбук к скринкасту можно открыть в Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Домашнее задание\n",
    "\n",
    "В домашнем задании вам предстоит решить задачу классификации одежды по датасету Fashion MNIST с помощью Keras.\n",
    "\n",
    "Для выполнения домашнего задания откройте Jupyter Notebook с инструкциями по решению задачи и сохраните его на свой Google Диск, с которого вы сможете работать на Google Colaboratory. \n",
    "\n",
    "Чтобы завершить домашнюю работу, введите свои ответы на вопросы в поля ниже. \n",
    "\n",
    "Удачи!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук с решением:  \n",
    "\n",
    "https://colab.research.google.com/drive/1NswpvRPhhrEPqwZjS08X0sex4d4R9RXI?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 13.1\n",
    "Обучите сеть без скрытых слоев (постройте аналог обычной линейной модели). Какое accuracy вы получили на валидации?  \n",
    "\n",
    "Ответ: ≥0.82 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 13.2\n",
    "Обучите сеть с двумя скрытыми слоями по 128 нейронов в каждом. Какое accuracy на валидации получили?  \n",
    "\n",
    "Ответ: ≥0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 13.3\n",
    "Сколько параметров в последней сети?  \n",
    "\n",
    "Ответ: 118282\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Знакомство с PyTorch\n",
    "\n",
    "**PyTorch** — такой же фреймворк автоматического дифференцирования, как и TensorFlow, поэтому, прежде чем приступать к знакомству с ним, рассмотрим, чем PyTorch отличается от последнего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дальнейшей работе с Deep Learning мы будем использовать именно PyTorch. Так в чём же его особенности?\n",
    "\n",
    "**TensorFlow ↓**\n",
    "\n",
    "- строим вычислительный граф из блоков;\n",
    "- вся тренировка нейросети происходит внутри метода fit();\n",
    "- нельзя контролировать промежуточные вычисления.  \n",
    "\n",
    "**PyTorch ↓**\n",
    "\n",
    "- центральное место разработки модели — цикл обучения;\n",
    "- шаги обучения прописываются самостоятельно;\n",
    "- можно контролировать промежуточные вычисления.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 14.1\n",
    "Почему в Tensorflow нет цикла обучения?  \n",
    "\n",
    "Ответ: Он есть внутри метода fit() и имплементирован на C++, потому недоступен извне "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Подробнее о тензорах\n",
    "**Тензор** — структура данных, содержащая числа в виде массивов разной размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность | Название\n",
    ":-- | :--\n",
    "__0__ |\tСкаляр \n",
    "__1__ |\tВектор\n",
    "__2__ |\tМатрица\n",
    "__3__ |\tТензор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Именно так, по факту тензором называется массив размерностью не только 3, но и любых других размерностей. Его можно инициализировать списком значений или массивом Numpy.\n",
    "\n",
    "PyTorch представляет более сотни разных операций над тензорами:\n",
    "\n",
    "- Индексирование и слайсинг\n",
    "- Арифметические операции: сложение, умножение, возведение в степень\n",
    "- Матричные операции: конкатенация, стекинг, транспонирование\n",
    "\n",
    "Давайте сделаем маленькое практическое задание для закрепления понимания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим очень важную операцию в глубоком обучении — матричное умножение. Важно не путать поэлементное умножение и матричное умножение.\n",
    "\n",
    "**В поэлементном умножении:**\n",
    "\n",
    "- размеры матриц всегда совпадают;\n",
    "- результат — матрица, где каждый элемент — произведение элементов из исходных матриц в соответствующих позициях.  \n",
    "\n",
    "**В матричном умножении:**\n",
    "\n",
    "- размеры матриц могут не совпадать;\n",
    "- в каждой ячейке результирующей матрицы — скалярное произведение векторов соответствующего столбца и строки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 15.1\n",
    "В ноутбуке вы видите два тензора, которые инициализированы как матрицы 4 х 4.\n",
    "\n",
    "Требуется выполнить три последовательных действия:\n",
    "вычислить выражение;\n",
    "сделать конкатенацию;\n",
    "посчитать скалярное произведение первой и последней строки получившегося тензора.\n",
    "Детально условия задания прописаны в ноутбуке. Результатом должно быть одно число.\n",
    "\n",
    "Ответ: 50  \n",
    "\n",
    "Ноутбук с решением здесь: https://colab.research.google.com/drive/14MvNh71f63g1ShKtQOUbJY5RukRxZFAa?usp=sharing\n",
    "\n",
    "### Задание 15.2\n",
    "Есть тензор из нулей размера 4 х 4\n",
    "\n",
    "Нужно заполнить третью строку единицами. Выберите подходящий вариант:\n",
    "\n",
    "Ответ: t1[2,:]=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Обучение нейросети\n",
    "\n",
    "Давайте теперь создадим полноценный скрипт обучения нейросети для решения задачи линейной регрессии.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 16.1\n",
    "Какое минимальное количество данных нужно взять для тренировки, чтобы отклонение от правильного значения b было не более 2%?  \n",
    "\n",
    "Ответ: > 500   \n",
    "\n",
    "### Задание 16.2\n",
    "Попробуйте в loss заменить MSE (среднеквадратичное отклонение) на MAE (абсолютное отклонение). Качество предсказаний после этого:  \n",
    "\n",
    "Ответ: Ухудшилось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Конструируем нейросеть из блоков\n",
    "Сейчас мы построим настоящую, «взрослую» нейросеть, выполняющую классификацию картинок одежды из датасета Fashion MNIST, с использованием которого вы уже делали классификатор на TensorFlow. Поскольку задача решается одна и та же, будет проще увидеть разницу между этими двумя фреймворками.\n",
    "\n",
    "\n",
    "Ноутбук: https://colab.research.google.com/drive/1xzAiIDppRvmcctHE-JrsEHlRMlOR96IU?usp=sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Домашнее задание\n",
    "В этом домашнем задании вам снова предстоит поиграть с параметрами задачи классификации одежды по датасету Fashion MNIST, теперь уже при помощи фреймворка PyTorch.\n",
    "\n",
    "Для выполнения домашнего задания откройте Jupyter Notebook из прошлого урока и сохраните его на свой Google Диск, с которого вы сможете работать на Google Colaboratory. \n",
    "\n",
    "Чтобы завершить домашнюю работу, введите свои ответы на вопросы в поля ниже. \n",
    "\n",
    "Удачи!\n",
    "\n",
    "### Задание 18.1\n",
    "Обучите сеть без последних двух полносвязных слоёв и дропаута. Какое accuracy на валидации у вас получилось?  \n",
    "\n",
    "Ответ: accuracy > 90%    \n",
    "\n",
    "### Задание 18.2\n",
    "Попробуйте увеличить learning rate до 0.01. Какое accuracy на валидации у вас получилось?  \n",
    "\n",
    "Ответ: 85% < accuracy < 90%    \n",
    "\n",
    "### Задание 18.3\n",
    "Сделайте замер accuracy не на тестовом сете, а на трейнсете. Какое значение получилось (округлите до процентов)?  \n",
    "\n",
    "Ответ: 92\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Заключение\n",
    "Вот и всё, друзья! ⭐\n",
    "\n",
    "Конечно, мы прошли лишь малую часть того, что можно сделать на PyTorch, но на этом мы ни в коем случае не прощаемся с нейросетями.\n",
    "\n",
    "Далее вас ждёт несколько недель профориентации, где вы сможете «потрогать» различные направления обучения и попробовать свои силы в них. А затем, в зависимости от специализации, вы будете изучать те или иные блоки, например свёрточные сети для компьютерного зрения и рекуррентные сети или блоки Attention для обработки естественного языка. \n",
    "\n",
    "На этом модуль завершается. Желаем вам успехов в выборе направления и дальнейшего постижения науки о глубоком обучении!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
