{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Юнит 8. Введение в нейронные сети\n",
    "### Skillfactory: DSPR-19\n",
    "### DL-2. Практика с основными фреймворками "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Уровни абстракции\n",
    "\n",
    "Самое время приступать к практике с основными фреймворками! В этом модуле мы научимся работать с двумя — **TensorFlow** и **PyTorch.**\n",
    "\n",
    "Но для начала разберёмся, насколько глубоко мы вообще можем погружаться в обучение нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует несколько уровней абстракции, на которых мы можем сконструировать нейросеть:\n",
    "\n",
    "- **Нижний уровень (NumPy)** — нет готовых высокоуровневых блоков, нет оптимизаторов, отсутствует возможность параллельных расчётов и использования GPU. Используется в учебных целях для иллюстрации принципов работы нейросетей.\n",
    "- **Фреймворки автоматического дифференцирования (PyTorch и TensorFlow)** — вы определяете вычислительный граф и некоторые гиперпараметры, а всё остальное фреймворк делает за вас. Позволяет строить нейросети произвольной сложности.\n",
    "- **Фреймворки готовых моделей (Transformers и DeepPavlov)** — скрывают технические детали самой нейросети и представляют высокоуровневый интерфейс для основных операций: обучения и инференса. Являются обёртками для больших и сложных нейросетей.\n",
    "\n",
    "### ФРЕЙМВОРКИ АВТОМАТИЧЕСКОГО ДИФФЕРЕНЦИРОВАНИЯ \n",
    "\n",
    "Рассмотрим, какие фреймворки представлены на рынке.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Вспоминаем линейную классификацию\n",
    "\n",
    "Прежде чем знакомиться с фреймворками, давайте снова поговорим о том, что такое линейная классификация. Это нам обязательно пригодится далее.\n",
    "\n",
    "### ЛИНЕЙНАЯ КЛАССИФИКАЦИЯ \n",
    "\n",
    "Представьте, что у нас есть признаки x = (x1, x2) и есть выборка положительных и отрицательных точек y ∈ {+1, −1}\n",
    "\n",
    "Нам нужно найти разделяющую гиперплоскость между ними. В данном случае это просто линия. Линия задаётся тремя коэффициентами:\n",
    "\n",
    "Нам нужно найти три коэффициента w, которые зададут линию. Далее мы можем взять точку х и понять, где она находится относительно линии: выше или ниже. Для этого нам нужно узнать знак линейной комбинации. Вектор весов w задаёт нормаль к нашей линии, то есть он перпендикулярен ей (фиолетовый вектор на графике ниже).\n",
    "\n",
    "Линейная комбинация — это скалярное произведение и длина проекции какого-нибудь другого вектора на наш вектор w.\n",
    "\n",
    "Поэтому проекция становится разных знаков. Из этих соображений мы делаем линейный классификатор. Наш алгоритм: \n",
    "\n",
    "Здесь появляется знак нашей линейной комбинации. Настроить линейный классификатор — значит найти эти коэффициенты. \n",
    "\n",
    "### ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ \n",
    "\n",
    "Она тоже решает задачу классификации, но в конце применяется не функция знака, а сигмоидная функция. Она превращает длину проекции в уверенность.\n",
    "\n",
    "Уверенность и неуверенность появляется из-за краевых эффектов: на границе классов может быть какой-то шум, и в классификации точек, которые находятся рядом с красной разделяющей линией, мы не очень уверены. \n",
    "\n",
    "А если мы уходим далеко от линии вглубь классов, то предполагается, что мы более уверены в этом предсказании. Сигмоида делает именно это — превращает длину проекции линейной комбинации в уверенность.\n",
    "\n",
    "Сигмоида устроена не случайным образом:\n",
    "\n",
    "Если длина проекции 0 (точка ровно на красной линии) , то сигмоида даёт . Логистическая регрессия предсказывает вероятность положительного класса. Вероятность отрицательного будет единица минус предсказанная вероятность положительного класса. \n",
    "\n",
    "#### ДРУГОЙ ПРИМЕР \n",
    "\n",
    "Представим, что у нас есть следующая задача:\n",
    "\n",
    "Чтобы решить подобную задачу, мы можем «подпереть» треугольник тремя линиями и сделать алгоритм, используя только логистическую регрессию.\n",
    "\n",
    "Для начала мы отделим минусы слева и построим логистическую регрессию:\n",
    "\n",
    "Эти данные подадим для обучения логистической регрессии и получим коэффициенты красной линии. \n",
    "\n",
    "Отделим остальные минусы:\n",
    "\n",
    "Все коэффициенты мы получили практически вручную. Сейчас у нас есть коэффициенты трёх логистических регрессий, каждая из которых решает свою маленькую подзадачу. \n",
    "\n",
    "Теперь возьмём какую-нибудь точку и посмотрим, какие три предсказания дают эти линии в точке:\n",
    "\n",
    "### ЧТО ДЕЛАТЬ С ЭТИМИ ТРЕМЯ ЗНАЧЕНИЯМИ? \n",
    "\n",
    "В координатах х1 и х2 эта задача не решается. Поэтому полученные коэффициенты мы можем рассматривать как новые координаты. \n",
    "\n",
    "Получим три признака, каждый из которых говорит, где мы находимся относительно каждой стороны треугольника. \n",
    "\n",
    "Давайте возьмём наш целевой признак y, добавим его в нашу новую таблицу, где наши новые признаки с предсказаниями, и попробуем решить её с новыми признаками с помощью линейной логистической регрессии. На новой выборке получим логистическую регрессию:\n",
    "\n",
    "Теперь она даёт нам некоторые коэффициенты и взвешивает уже не признаки, а предсказания. То, что мы получили — простейшая нейросеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.1\n",
    "Почему в логистической регрессии используется Сигмоида, а не Знак (sign)?  \n",
    "\n",
    "Ответ: Сигмоида плавная и непрерывная, что позволяет её дифференцировать верно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Граф вычислений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент мы «руками» нашли параметры всех линий:\n",
    "\n",
    "При этом на нашу сложную комбинацию функций можно посмотреть как на граф вычислений. У графа есть вершины, и каждая вершина — вычисляемое значение.\n",
    "\n",
    "Также есть рёбра — зависимости, которые имеют направления (на картинке ниже это стрелочки). Ребро идёт от х1 к z1 в случае, если нам необходим х1, чтобы вычислить значение z1. Это граф зависимости между вычисляемыми значениями:\n",
    "\n",
    "Граф соответствует комбинации наших функций. Такой граф называют многослойным персептроном, и здесь уже можно видеть некоторые слои:\n",
    "\n",
    "- входной слой (признаки);\n",
    "- скрытый слой (нейроны);\n",
    "- выходной слой (предсказания).\n",
    "\n",
    "### НЕЛИНЕЙНОСТИ В НЕЙРОНАХ \n",
    "\n",
    "Возьмём в качестве примера следующий граф:\n",
    "\n",
    "Если нелинейности убрать, то на этом примере видно, что наша модель станет очень простой: мы можем подставить выражения для z1 и z2 в нашу модель а:\n",
    "\n",
    "Мы можем раскрыть скобки, привести подобные слагаемые и всё, что мы получим — линейную комбинацию х1 и х2. При этом модель сложнее не становится.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.1\n",
    "Зачем перцептроны делают многослойными?  \n",
    "\n",
    "Ответ: Несколько слоёв позволяют разделить данные, которые не делятся одной линией "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.2\n",
    "Что будет, если убрать нелинейность из скрытого слоя?  \n",
    "\n",
    "\n",
    "Ответ:  \n",
    "- Скрытого слоя как бы не станет\n",
    "- Модель не сможет работать со сложными данными, не делящимися одной линией\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MLP\n",
    "\n",
    "**MLP** — это простейший пример нейросети:\n",
    "\n",
    "Слои в MLP называют:\n",
    "\n",
    "- Dense layer (плотный);\n",
    "- Fully-connected layer (полносвязный).\n",
    "\n",
    "### Архитектура MLP:\n",
    "\n",
    "- количество слоёв;\n",
    "- количество нейронов в каждом слое;\n",
    "- функция активации, которую будем использовать.\n",
    "\n",
    "\n",
    "### Задание 4.1\n",
    "\n",
    "Какие из параметров обучаемые, а какие являются гиперпараметрами нейросети?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр | Вид\n",
    ":-- | :--\n",
    "__Веса связей между входными нейронами и слоем Z__ |\tОбучаемые параметры \n",
    "__Количество нейронов в слое Z__ |\tгиперпараметры\n",
    "__Функция активации в слое Z__ |\tгиперпараметры\n",
    "__Веса связей между слоем Z и слоем H__ |\tОбучаемые параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. TensorFlow\n",
    "\n",
    "**TENSORFLOW (TF) — DEEP LEARNING ФРЕЙМВОРК**\n",
    "\n",
    "Основа вычислений в **TF — граф**. Каждая вершина графа — это одна операция, у которой есть входы и выходы.\n",
    "\n",
    "- Вход любой операции — это набор тензоров (многомерных массивов).\n",
    "- Выход любой операции — это тоже набор тензоров.  \n",
    "\n",
    "А у нас целый граф операций, между которыми перекидываются тензоры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительные материалы:\n",
    "https://www.oreilly.com/content/hello-tensorflow/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5.1\n",
    "Тензором являются:\n",
    "- данные, поступающие на вход\n",
    "- веса связей между слоями\n",
    "- ошибка предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
